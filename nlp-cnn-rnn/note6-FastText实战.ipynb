{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Input,Model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train_lenth\n",
      "25000 train_lenth\n",
      "Average train sequence length:238\n",
      "Average test sequence length:230\n",
      "pad sequences....\n",
      "x_train: (25000, 400)\n",
      "x_test: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 400\n",
    "max_features = 2000\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data('./imdb.npz',num_words=max_features)\n",
    "\n",
    "print(len(x_train),'train_lenth')\n",
    "print(len(x_test),'train_lenth')\n",
    "\n",
    "print('Average train sequence length:{}'.format(np.mean(list(map(len,x_train)),dtype=int)))\n",
    "print('Average test sequence length:{}'.format(np.mean(list(map(len,x_test)),dtype=int)))\n",
    "\n",
    "#填充为同一长度\n",
    "print('pad sequences....')\n",
    "x_train = sequence.pad_sequences(x_train,maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test,maxlen = maxlen)\n",
    "print('x_train:',x_train.shape)\n",
    "print('x_test:',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 50\n",
    "\n",
    "class FastText(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims, class_num=1, \n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "    \n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length = self.maxlen)(input) \n",
    "        #mbedding的主要目的是对（稀疏）特征进行降维，它降维的方式可以类比为一个全连接层（没有激活函数），通过 embedding 层的权重矩阵计算来降低维度。\n",
    "        x = GlobalAveragePooling1D()(embedding)\n",
    "        # GlobalAveragePooling：把特征图全局平均一下输出一个值，也就是把W*H*D的一个张量变成1*1*D的张量\n",
    "        '''如在平均池化前的张量输出是8x8x1024，对每个8x8的特征图做一个平均池化(取一个平均数)，就可以得到1024个标量了，\n",
    "        然后在进入一个1000结点的全连接层，最后通过softmax输出'''\n",
    "        output = Dense(self.class_num, activation = self.last_activation)(x) #全连接层\n",
    "        model = Model(inputs = input, outputs = output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构造模型...\n",
      "训练模型.....\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 16s 648us/step - loss: 0.6246 - acc: 0.7211 - val_loss: 0.5244 - val_acc: 0.8024\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 15s 596us/step - loss: 0.4421 - acc: 0.8398 - val_loss: 0.3920 - val_acc: 0.8552\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 15s 610us/step - loss: 0.3558 - acc: 0.8654 - val_loss: 0.3458 - val_acc: 0.8625\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 15s 609us/step - loss: 0.3186 - acc: 0.8758 - val_loss: 0.3218 - val_acc: 0.8720\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 15s 607us/step - loss: 0.2981 - acc: 0.8817 - val_loss: 0.3086 - val_acc: 0.8763\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 15s 614us/step - loss: 0.2839 - acc: 0.8877 - val_loss: 0.3011 - val_acc: 0.8779\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 15s 605us/step - loss: 0.2742 - acc: 0.8921 - val_loss: 0.2985 - val_acc: 0.8777\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 15s 608us/step - loss: 0.2674 - acc: 0.8938 - val_loss: 0.2956 - val_acc: 0.8796\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 15s 610us/step - loss: 0.2625 - acc: 0.8947 - val_loss: 0.2944 - val_acc: 0.8798\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 15s 608us/step - loss: 0.2570 - acc: 0.8974 - val_loss: 0.2963 - val_acc: 0.8785\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 15s 609us/step - loss: 0.2540 - acc: 0.8983 - val_loss: 0.2994 - val_acc: 0.8748\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 15s 617us/step - loss: 0.2519 - acc: 0.8991 - val_loss: 0.2967 - val_acc: 0.8790\n",
      "模型测试.......\n",
      "test data accuracy is: 0.87896\n"
     ]
    }
   ],
   "source": [
    "batch_size =32\n",
    "epochs =20\n",
    "\n",
    "print('构造模型...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam','binary_crossentropy',metrics = ['accuracy'])\n",
    "#keras model.compile(loss='目标函数 ', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print('训练模型.....')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience = 3, mode = 'max') \n",
    "#使用了acc验证，故mode取max\n",
    "#有验证集 故monitor选取val_acc 而不是acc\n",
    "'''EarlyStopping参数说明： https://blog.csdn.net/silent56_th/article/details/72845912'''\n",
    "model.fit(x_train,y_train,\n",
    "         batch_size = batch_size,\n",
    "         epochs = epochs,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data = (x_test,y_test))\n",
    "\n",
    "print('模型测试.......')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('test data accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram 增加二元词组信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value =2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(1,4),(4, 9), (9, 4), (4, 1)}\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range+1):\n",
    "            for i in range(len(new_list)-ngram_value+1):\n",
    "                ngram = tuple(new_list[i:i+ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train_lenth\n",
      "25000 train_lenth\n",
      "Average train sequence length:238\n",
      "Average test sequence length:230\n",
      "add 2-gram features.....\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 452\n",
      "********************\n",
      "pad sequences....\n",
      "x_train: (25000, 400)\n",
      "x_test: (25000, 400)\n",
      "********************\n",
      "构造模型...\n",
      "********************\n",
      "训练模型.....\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 380s 15ms/step - loss: 0.5886 - acc: 0.7607 - val_loss: 0.4503 - val_acc: 0.8424\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 358s 14ms/step - loss: 0.3334 - acc: 0.8971 - val_loss: 0.3199 - val_acc: 0.8798\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 398s 16ms/step - loss: 0.2119 - acc: 0.9373 - val_loss: 0.2774 - val_acc: 0.8904\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 404s 16ms/step - loss: 0.1438 - acc: 0.9616 - val_loss: 0.2575 - val_acc: 0.8966\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 396s 16ms/step - loss: 0.0979 - acc: 0.9766 - val_loss: 0.2500 - val_acc: 0.8977\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 386s 15ms/step - loss: 0.0665 - acc: 0.9870 - val_loss: 0.2482 - val_acc: 0.8998\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 543s 22ms/step - loss: 0.0446 - acc: 0.9933 - val_loss: 0.2517 - val_acc: 0.8992\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 358s 14ms/step - loss: 0.0299 - acc: 0.9963 - val_loss: 0.2560 - val_acc: 0.9002\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 372s 15ms/step - loss: 0.0200 - acc: 0.9980 - val_loss: 0.2645 - val_acc: 0.8984\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 2392s 96ms/step - loss: 0.0133 - acc: 0.9990 - val_loss: 0.2748 - val_acc: 0.8981\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 12468s 499ms/step - loss: 0.0090 - acc: 0.9993 - val_loss: 0.2870 - val_acc: 0.8960\n",
      "********************\n",
      "模型测试.......\n",
      "test data accuracy is: 0.896\n"
     ]
    }
   ],
   "source": [
    "ngram_range = 2\n",
    "max_features = 2000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 20\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data('./imdb.npz',num_words=max_features)\n",
    "\n",
    "print(len(x_train),'train_lenth')\n",
    "print(len(x_test),'train_lenth')\n",
    "\n",
    "print('Average train sequence length:{}'.format(np.mean(list(map(len,x_train)),dtype=int)))\n",
    "print('Average test sequence length:{}'.format(np.mean(list(map(len,x_test)),dtype=int)))\n",
    "\n",
    "\n",
    "if ngram_range>1:\n",
    "    print('add {}-gram features.....'.format(ngram_range))\n",
    "    #Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2,ngram_range+1):\n",
    "            set_of_ngram =  create_ngram_set(input_list, ngram_value = i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "     \n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features +1\n",
    "    token_indice = {v:k+start_index for k,v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "    \n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys()))+1\n",
    "    \n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "    \n",
    "\n",
    "print('*'*20)\n",
    "#填充为同一长度\n",
    "print('pad sequences....')\n",
    "x_train = sequence.pad_sequences(x_train,maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test,maxlen = maxlen)\n",
    "print('x_train:',x_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "\n",
    "print('*'*20)\n",
    "print('构造模型...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam','binary_crossentropy',metrics = ['accuracy'])\n",
    "#keras model.compile(loss='目标函数 ', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('*'*20)\n",
    "print('训练模型.....')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience = 3, mode = 'max') \n",
    "#使用了acc验证，故mode取max\n",
    "#有验证集 故monitor选取val_acc 而不是acc\n",
    "'''EarlyStopping参数说明： https://blog.csdn.net/silent56_th/article/details/72845912'''\n",
    "model.fit(x_train,y_train,\n",
    "         batch_size = batch_size,\n",
    "         epochs = epochs,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data = (x_test,y_test))\n",
    "\n",
    "print('*'*20)\n",
    "print('模型测试.......')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('test data accuracy is:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
